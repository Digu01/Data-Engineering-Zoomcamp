{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f117ceb9-a9d9-45d4-8a53-227fd84b3895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HADOOP_HOME'] = 'C:/tools/hadoop-3.2.0'\n",
    "os.environ['JAVA_HOME'] = 'C:/tools/jdk-11.0.21'\n",
    "os.environ['SPARK_HOME'] = 'C:/tools/spark-3.3.2-bin-hadoop3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "040d3919-773f-4d4b-b5f5-d3dca59d7f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd17ae24-8af0-48ce-909b-4eb6bfe35629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.conf.SparkConf object at 0x00000298F57A2ED0>\n"
     ]
    }
   ],
   "source": [
    "#set up your configurations before building your SparkSession.\n",
    "\n",
    "google_credentials = \"C:/Users/LENOVO/Desktop/Data-Engineering-Zoomcamp/Module5-Batch_Processing_Spark\\HW/module5-spark-2534760391ac.json\"\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('spark://localhost:7077') \\\n",
    "    .setAppName('test') \\\n",
    "    .set(\"spark.jars\", \"C:/tools/spark-3.3.2-bin-hadoop3/jars/gcs-connector-hadoop3-latest.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", google_credentials)\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2428f5d0-b805-498a-b0d2-ef62a4ba7033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=spark://localhost:7077 appName=Spark_connect_gcs_App>\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext('spark://localhost:7077', \"Spark_connect_gcs_App\")\n",
    "\n",
    "# set google configurations inside SparkSession\n",
    "hadoop_config = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_config.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_config.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_config.set(\"fs.gs.auth.service.account.json.keyfile\", google_credentials)\n",
    "hadoop_config.set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
    "\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ac6a268-7faf-4a6d-9194-bada72e8dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337dbc46-136c-4fcb-a973-3d75d219048d",
   "metadata": {},
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd4f9db-b304-459f-ad0a-d82ef0121349",
   "metadata": {},
   "source": [
    "Read the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons.\n",
    "\n",
    "Repartition the Dataframe to 6 partitions and save it to parquet.\n",
    "\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f6b2a33-c7b0-4246-9e90-57772a51acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Define schema\n",
    "from pyspark.sql import types\n",
    "fhv_schema = types.StructType([\n",
    "    types.StructField(\"dispatching_base_num\", types.StringType(), True),\n",
    "    types.StructField(\"pickup_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"dropOff_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"PULocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"DOLocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"SR_Flag\", types.DoubleType(), True),\n",
    "    types.StructField(\"Affiliated_base_number\", types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f3f199f-a8e1-4264-8aa4-76aedc5e44c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: double (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read csv file as spark dataframe & set schema\n",
    "fhv_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(fhv_schema)\\\n",
    "    .csv(\"gs://spark-output-bi-reports/dataset/fhv_tripdata_2019-10.csv\")\n",
    "\n",
    "fhv_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f8555a9-68c1-49bb-a1f3-abd85fdec514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. partition data to 6 partitions\n",
    "fhv_partitioned = fhv_df.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00c90a47-dfcc-40bb-8493-3e022baf40c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_partitioned.write.mode('overwrite').parquet(\"C:/Users/LENOVO/Desktop/Data-Engineering-Zoomcamp/Module5-Batch_Processing_Spark/HW/output_partitioned/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe2a4de-8b2f-42a3-8211-b8e2dd336aea",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "Count records\n",
    "\n",
    "How many taxi trips were there on the 15th of October?\n",
    "\n",
    "Consider only trips that started on the 15th of October."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3569c769-f7e9-4505-a1e2-ab4f5be09653",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_partitioned.createOrReplaceTempView('fhv_oct_trips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5cc81119-f558-4f1d-9c13-afd76a362b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   62610|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_trips_middle_oct = spark.sql(\"\"\"\n",
    "select \n",
    "    count(*)\n",
    "from \n",
    "    fhv_oct_trips\n",
    "where \n",
    "    extract(DAY from pickup_datetime) = 15\n",
    "\"\"\")\n",
    "count_trips_middle_oct.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3590727f-7d7b-456c-a587-3dd14fbaf975",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "Longest trip for each day\n",
    "\n",
    "What is the length of the longest trip in the dataset in hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4932c4b7-6295-44a5-b742-02a885e83387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|    pickup_datetime|   dropOff_datetime|longest_trip_length|\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2019-10-28 09:00:00|2091-10-28 09:30:00|             631152|\n",
      "+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "longest_trip = spark.sql(\"\"\"\n",
    "select \n",
    "    pickup_datetime, dropOff_datetime, DATEDIFF(hour, pickup_datetime, dropOff_datetime) as longest_trip_length\n",
    "from \n",
    "    fhv_oct_trips\n",
    "order by 3 desc\n",
    "limit 1\n",
    "\"\"\")\n",
    "longest_trip.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445933fb-a9d6-4d84-a1d2-a60f4549dfe8",
   "metadata": {},
   "source": [
    "Question 6:\n",
    "Least frequent pickup location zone\n",
    "\n",
    "Load the zone lookup data into a temp view in Spark\n",
    "Zone Data\n",
    "\n",
    "Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3633642e-f2f7-4660-aa0b-54f04abc4064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define zones schema\n",
    "zones_schema = types.StructType([\n",
    "    types.StructField(\"LocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"Borough\", types.StringType(), True),\n",
    "    types.StructField(\"Zone\", types.StringType(), True),\n",
    "    types.StructField(\"service_zone\", types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc44ce11-ce0e-4398-936a-01fdbe7b8afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read csv file as spark dataframe & set schema\n",
    "zones_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(zones_schema)\\\n",
    "    .csv(\"dataset/taxi+_zone_lookup.csv\")\n",
    "\n",
    "zones_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4387f5c-67f4-482b-8756-241f4282b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_df.createOrReplaceTempView(\"zones_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7a417749-d8db-467f-ae89-8e47ac1f1ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|       Zone|least_zone_pickup|\n",
      "+-----------+-----------------+\n",
      "|Jamaica Bay|                1|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_data = spark.sql(\"\"\"\n",
    "select \n",
    "    zones.Zone, count(*) as least_zone_pickup\n",
    "from \n",
    "    fhv_oct_trips as fhv_trips\n",
    "join \n",
    "    zones_table as zones\n",
    "on \n",
    "    fhv_trips.PULocationID = zones.LocationID\n",
    "group by \n",
    "    1\n",
    "order by \n",
    "    2 asc\n",
    "limit \n",
    "    1\n",
    "\"\"\")\n",
    "\n",
    "joined_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bfebcd-b9c8-4717-bef6-b55ea584d780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
